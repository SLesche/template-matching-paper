# General discussion
Manual extraction of ERP latencies has so far proven more reliable and valid than algorithmic approaches [@sadus2024explorative] but presents a time- and resource-intensive process. The newly proposed template matching algorithm replicated human behavior and recovered simulated latency shifts better than any of the previous approaches like peak latency or fractional area latency. Template matching algorithms using normalized weights showed a mean intraclass correlation (ICC) with manually extracted P3 latencies of `r mean_icc_normalized_kathrin` across tasks, filter settings, similarity measures, weighting windows, and penalties. This indicates that template matching algorithms are able to replicate manual extraction accurately while presenting a more objective and efficient approach to latency extraction. In addition, template matching algorithms were able to closely recover simulated latency shifts. Template matching algorithms were best at recovering true latency shifts (`r mean_icc_normalized_simulation`) across filter settings, similarity measures, weighting windows, and penalties. Application of template matching algorithms based on the grand average can increase replicability and scalability. Furthermore, it significantly reduces the time and resources spent on latency extraction while proving consistently reliable and valid. 

The MINSQ and the MAXCOR approach in combination with normalized weights and a penalty for extreme values worked well across tasks, filter settings and weighting windows. Nonetheless, the exact specification of similarity measures, weighting windows, weighting functions, and penalties depends on a variety of factors.

## MAXCOR vs. MINSQ
Both the template matching algorithm maximizing the correlation (MAXCOR) and the algorithm minimizing the squared distance between template and signal (MINSQ) outperformed previous approaches. In both the simulation and the empirical data, the MAXCOR approach lead to fewer cases where no latency could be specified than the MINSQ approach. We attribute this to cases where few parts of the signal have positive-going amplitude in the P3 window. In this case, the MINSQ approach fails to find a proper match because the optimal transformation would include $a = 0$. We did prevent this by setting a lower bound of $0.2$, which will force the optimization to try and find a more reasonable match. However, this issue does prevail in some cases, leading to bad fit statistics and thus resulting in missing values. The MAXCOR approach does not suffer from this issue because the value of the amplitude is irrelevant to this approach. Here, we can not recover any sensible parameters for $a$. In future applications where $a$ might be used to measure the amplitude of a component, the MAXCOR algorithm would prove useless.

Focusing only on latencies, the MAXCOR approach slightly outperformed the MINSQ approach in replicating an expert ERP researcher. On the other hand, the MINSQ approach displayed higher reliability in the empirical data and showed better recovery in the simulation. Importantly, even expert ERP researchers are not perfect when extracting ERP latencies. Therefore, we place more emphasis on the results of the simulation, where the MINSQ approach gained the upper hand. In practice, both approaches performed very well and applying them will lead to improved quality in the extracted latencies. The tradeoff between a higher amount of missing values or slightly lower validity should be tailored to the specific requirements of the research project. In cases where large amounts of data are available or analysis methods are less affected by missing values, the MINSQ approach seems to prove best.

## Weighting functions
We tested four different weighting functions. Not applying any weighting function did perform well in the simulation but performed poorly in empirical data. The Hamming and Tukey weighting functions performed poorly in both the empirical data and the simulation. Using the maximum-normalized amplitude in the template to generate weights lead to the best results across the board. While the exact shape of the weighting function is open to discussion, these results support the notion that a weighting function that is based on the amplitude in the template itself will lead to the best results. We argue that these normalized weights are also the closest formalization of the process a human researcher employs when identifying ERP component latencies. The shape around the peak of the component is most important and the shape around the onset and offset of the components are less important. If there is a pronounced surrounding component, like the N2 in our case, researchers may use this as an additional indicator. The normalized weighting function best captures this behaviour.

## Weighting windows
We investigated three different weighting windows in order to gauge the impact this has on our template matching algorithms. For both approaches and across tasks and filter settings, the weighting window did not impact the validity greatly. In some cases, the wide window (250 - 900ms) seems to lead to slightly worse validity than the other windows. We therefore recommend specifying a weighting window in a similar manner to a measurement window for an area-based latency algorithm. Any window that includes a part of the onset, the peak and a part of the offset should perform reasonably well. 

This invariance to the weighting window is an additional benefit of template matching approaches. In contrast to traditional algorithms, the weighting window is not applied to each subject level ERP. Measurement windows, as used in traditional algorithms, may not include the component of interest fully in a given subject-level ERP. Often, it is then the expert's job to readjust this measurement window for each individual. In the template matching algorithms, the weighting window is only applied to the template and then used to generate the weighing vector. Therefore, it will always include exactly those parts of the component that were previously specified by the researcher.

## Penalty
Inclusion of a penalty term for extreme values of $b$ during the optimization reduced the percentage of missing values greatly. We suspect that this is because extreme values are often the result of spurious high matches with very late or very early parts of the signal. This can occur in cases where the signal to noise ratio of a subject's ERP is low. Forcing the algorithm to converge on a more typical value thus leads to more reasonable latency estimates that also still generate acceptable fit statistics. Importantly, this is a penalty and not an upper/lower bound. The algorithm can converge on extreme values in cases where the latency of the component in the subject does deviate from the template greatly and less extreme values of $b$ do not lead to significantly better similarity measures. Additionally, no penalty is being applied as long as $0.\overline{6} \le b \le 1.5$. Accordingly, no bias towards the mean is present during the optimization process for most ERPs.

A bias towards the mean would manifest in a reduction in validity. We did observe a reduction in validity in the simulation, but only for the MINSQ approach. In the empirical evaluation, the MINSQ approach matched the expert more closely when a penalty was used. The MAXCOR approach had higher validity in both the simulation and the empirical evaluation. Therefore, we see no signs of a reduction in validity that a bias towards the mean would entail.

In practice, whether to apply a penalty should be determined by the degree of automation needed and the amount of missing values that are tolerable. If there is no time to review cases with extreme values of $b$ and missing values have to be avoided, a penalty term should be applied during optimization. This ensures a lower number of missing values without reducing validity. In order to ensure optimal validity, we suggest running the algorithm without a penalty term and then manually reviewing those cases with the worst fit statistics and the most extreme values of $b$.

## Fit statistic
Template matching algorithms allow researchers to selectively review those subject-level ERPs where the correlation between template and signal indicates low similarity. For example, researchers can use the algorithm to  efficiently and objectively extract the majority of ERP latencies and only spend time on those cases with the worst fit statistics. Here, a human may be able to quantify some ERPs that the algorithm fails on, thus increasing the amount of usable data. By adjusting the cutoff after which manual review is necessary, researchers can be as liberal or conservative as they choose. The algorithm performs well without a review, but the validity of the extracted data will probably increase after human inspection. 

No other algorithms allow this type of efficient manual intervention or adjustable automatic rejection of the worst cases. In traditional approaches, subject-level metrics of data quality, like the standardized measurement error (SME) [@luck2021standardized] may be used to exclude those subjects with insufficient quality metrics [@wascher2022mental]. This does lead to improved results and may be considered as an alternative. However, the SME is directly influence by the number of trials [@zhang2023variations] and the number of data-points [@rodrigues2024exploration]. The fit statistic generated by template matching approaches is unaffected by both of them. Furthermore, the correlation between template and signal provides a more intuitive quantification of data quality. It directly reflects the extent to which the component of interest is present in the data and how much it is corrupted by noise.

Importantly, noise in the sense of correlation between template and signal is any unsystematic difference in the covariance term between the signal and the transformed template. Trial-to-trial variability, which may be an important part in the neurocognitive  processes underlying the task, will not influence the fit statistic as long as this variability occurs in most subjects. While the SME would classify this as noise, the fit statistic of template matching algorithm allows for intertrial variability. For example, variability in the true latency of the component will lead to broader components in both the subject-level ERPs as well as the template. 

## Limitations and future research
Most importantly, this work is limited in scope. As a first step, we used template matching algorithms to extract P3 latencies only. In order to improve generalizability, we included three different tasks and varied the low-pass filter during preprocessing. While the different template matching approaches differed in validity between the tasks and filter settings, we did not observe severe issues in any of these 15 different conditions. We are confident that template matching algorithms can be used to extract P3 latencies in other tasks or using other preprocessing pipelines. Nonetheless, we did not investigate any other components and will remedy this shortcoming in future work.

Secondly, application of the template matching algorithm depends heavily on the template itself. If there is no clear component structure in the grand average, a template matching algorithm will not be of any help. However, we suspect that other, more problematic issues are at hand in this case. 

Additionally, application of a template matching algorithm is more complex and computationally demanding than the previous algorithms. We want to address the complexity issue in future work and simplify the application of template matching algorithms to make them more accessible. Quantification of a single ERP takes around 0.05-0.2 seconds depending on the hardware. For extremely large data and low computing power, using the fractional area latency algorithm with the modifications proposed by @liesefeld2018estimating may be the better choice. However, with access to a high performance computing cluster running 25 nodes in parallel, we were able to run the quantification of 4.8 million ERPs in under 24 hours.

Lastly, we used an arbitrary cutoff for the fit statistic that was based on our previous experience in working with the algorithm. As with any cutoff, the exact value is up to debate and can be specified by the researcher on a study-by-study basis.

## Conclusion
Template matching algorithms using the grand average as a dynamic template were able to extract P3 latencies better than any previous algorithm both in empirical data as well as in our simulation. Template matching algorithms show the best correlation with latencies extracted by an expert researcher and the best recovery of true simulated latency shifts. Both the MAXCOR and MINSQ approach in combination with normalized weights work well across tasks, filter settings, weighting windows, and penalties. The algorithms also provide a fit statistic that may be used to automatically discard those ERPs with low matches to the template. It may also be used to flag certain ERPs for later manual review.

The exact specification of which similarity measure to use, which weighting function to apply, whether to apply a penalty, and which cutoff to use for the fit statistic is up to each individual researcher. We recommend using the MINSQ approach with normalized weights and a weighting window that includes the on- and offset of the component. If no manual inspection is possible, we recommend using a penalty for extreme values of $b$ during the optimization process. Otherwise, we recommend using the unpenalized MINSQ approach and manually reviewing only cases with the worst fits and most extreme values of $b$. 

We showed that this template matching algorithm improves objectivity and efficiency while maintaining consistently good reliability and superior validity over previous approaches. This algorithm could be used as a final step for latency estimation in multiverse studies [@sadus2024explorative; @zhang2023variations] and other automated pipelines [@wascher2022mental; @clayson2021data; @rodrigues2021epos]. We hope to demonstrate the efficacy of template matching algorithms for other components in future work.

<!-- Notes -->
<!-- Hier Diskussion aufzeigen: -->
<!-- 	- Maxcor vs. minsq -->
<!-- 	  Both work better than previous approaches, MAXCOR a little better than MINSQ in empirical, MINSQ better in simulation. Decision should come down to number of missing values and whether amplitudes are of interest -->
<!-- 	- normalized weights vs. no weights (correlations with other components, outlook for other components) -->
<!-- 	  Weight has large impact. Hamming/Tukey/None show significantly worse values -->
<!-- 	- wide window vs. other windows -->
<!-- 	- penalty vs. none (missing values!) -->
<!-- 	  Penalty has impact on missing values, and slightly on ICC in empirical data -->
<!-- 	  Say that missing values in Human data was X, so peak probably classifies some that should be missing -->


<!-- - Discuss Simulation approach and benefits/drawbacks. say that you would pay more attention to simulatino -->
<!-- - Dann Limitations: Complexity, Time-Investment, Generalizability to other components, missing values, arbitrary cutoff for fit statistic -->

<!-- - Future research: Manual + Algorithms, review based on fit statistic; other components  + Recovering Amplitude. Using the derivative for template matching -->
